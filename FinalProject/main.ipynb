{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from scipy import ndimage as ndimage\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, we will be implementing a neural network model for gesture detection. The input to the model will be a\n",
    "tensor and shape (batch_size, duration, n_channels). Each hand skeleton will have 22 joints, and each joint will be 3*number of joints\n",
    "channels over the time.\n",
    "\n",
    "To extract features from the input data, we will first process each channel separately. We will use 1D convolutions to\n",
    "process each channel, and the neural network will consist of three convolutional layers and pooling layers.\n",
    "\n",
    "The output of each convolutional layer will be concatenated into a single output, which will be used as input for the next \n",
    "layer. Finally, the three outputs will be concatenated into one output.\n",
    "\n",
    "The neural network architecture for this model can be summarized as follows:\n",
    "\n",
    "Input layer with shape (batch_size, duration, n_channels)\n",
    "Three 1D convolutional layers with padding, followed by a max pooling layer\n",
    "Three output layers for each channel, with a concatenation layer at the end\n",
    "By using 1D convolutions and pooling layers, we can extract meaningful features from the\n",
    "hand skeleton data. The concatenation layer at the end allows us to combine the information from each channel \n",
    "into a single output, which can be used to predict the gesture being performed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed data, you can also use another.pckl file\n",
    "def load_data(filepath):\n",
    "    file = open(filepath, 'rb')\n",
    "    data = pickle.load(file, encoding='latin1') \n",
    "    file.close()\n",
    "    return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']\n",
    "\n",
    "def preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "    x_train, x_test = resize_sequences_length(x_train, x_test, final_length=100)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensor type\n",
    "def convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = numpy.array(y_train_14), numpy.array(y_train_28), numpy.array(y_test_14), numpy.array(y_test_28)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14 - 1, y_train_28 - 1, y_test_14 - 1, y_test_28 - 1\n",
    "    x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = torch.from_numpy(y_train_14), torch.from_numpy(y_train_28), torch.from_numpy(y_test_14), torch.from_numpy(y_test_28)\n",
    "    x_train, x_test = x_train.type(torch.FloatTensor), x_test.type(torch.FloatTensor)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14.type(torch.LongTensor), y_train_28.type(torch.LongTensor), y_test_14.type(torch.LongTensor), y_test_28.type(torch.LongTensor)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_sequences_length(x_train, x_test, final_length=100):\n",
    "    x_train = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1))]).T for x_i in x_train])\n",
    "    x_test  = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1)) ]).T for x_i in x_test])\n",
    "    return x_train, x_test\n",
    "\n",
    "def shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    x_train, y_train_14, y_train_28 = shuffle(x_train, y_train_14, y_train_28)\n",
    "    x_test,  y_test_14,  y_test_28  = shuffle(x_test,  y_test_14,  y_test_28)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandGestureNet(torch.nn.Module): \n",
    "    \"\"\"\n",
    "    citation:\n",
    "    ------------\n",
    "        @inproceedings{devineau2018deep,\n",
    "            title={Deep learning for hand gesture recognition on skeletal data},\n",
    "            author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
    "            booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
    "            pages={106--113},\n",
    "            year={2018},\n",
    "            organization={IEEE}\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=66, n_classes=14, dropout_probability=0.2):\n",
    "# Layers\n",
    "        super(HandGestureNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_probability = dropout_probability\n",
    "\n",
    "        self.all_conv_high = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_conv_low = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_residual = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=9 * n_channels * 12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
    "        )\n",
    "# process each channel\n",
    "        for m in itertools.chain(self.all_conv_high, self.all_conv_low, self.all_residual):\n",
    "            for layer in m:\n",
    "                if layer.__class__.__name__ == \"Conv1d\":\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                    torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "        for ll in self.fc:\n",
    "            if ll.__class__.__name__ == \"Linear\":\n",
    "                torch.nn.init.xavier_uniform_(ll.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "#     def next(self, input):\n",
    "#         all_features = []\n",
    "# # compute forward of the network\n",
    "#         for channel in range(0, self.n_channels):\n",
    "#             cc = input[:, :, channel]\n",
    "#             cc = cc.unsqueeze(1)\n",
    "#             low = self.all_conv_low[channel](cc)\n",
    "#             high = self.all_conv_high[channel](cc)\n",
    "            \n",
    "#             a_residual = self.all_residual[channel](cc)\n",
    "\n",
    "#             output_channel = torch.cat([high,low,a_residual], dim=1)\n",
    "#             all_features.append(output_channel)\n",
    "\n",
    "#         all_features = torch.cat(all_features, dim=1)\n",
    "#         all_features = all_features.view(-1, 9 * self.n_channels * 12)  # <-- 12: depends of the initial sequence length (100).\n",
    "#         output = self.fc(all_features)\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(tensor, batch_size=32):\n",
    "    tensor_list = []\n",
    "    length = tensor.shape[0]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if (i + 1) * batch_size >= length:\n",
    "            tensor_list.append(tensor[i * batch_size: length])\n",
    "            return tensor_list\n",
    "        tensor_list.append(tensor[i * batch_size: (i + 1) * batch_size])\n",
    "        i += 1\n",
    "# get training time\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m * 60\n",
    "    return '{:02d}m {:02d}s'.format(int(m), int(s))\n",
    "# get accuracy\n",
    "def acc_of(model, x, y_ref):\n",
    "    acc = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted = model(x)\n",
    "        _, predicted = predicted.max(dim=1)\n",
    "        acc = 1.0 * (predicted == y_ref).sum().item() / y_ref.shape[0]\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except:\n",
    "    class SummaryWriter():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer,\n",
    "          x_train, y_train, x_test, y_test,\n",
    "          force_cpu=False, num_epochs=5):\n",
    "#     Check if using a GPU\n",
    "  \"\"\"\n",
    "    citation:\n",
    "    ------------\n",
    "        @inproceedings{devineau2018deep,\n",
    "            title={Deep learning for hand gesture recognition on skeletal data},\n",
    "            author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
    "            booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
    "            pages={106--113},\n",
    "            year={2018},\n",
    "            organization={IEEE}\n",
    "        }\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available() and not force_cpu:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.has_mps:\n",
    "        device = torch.device('mps')\n",
    "    else: \n",
    "        device = torch.device(\"cpu\")\n",
    "#     put to device\n",
    "    model = model.to(device)\n",
    "    x_train, y_train, x_test, y_test = x_train.to(device), y_train.to(device), x_test.to(device), y_test.to(device)\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    x_train_batches = batch(x_train)\n",
    "    y_train_batches = batch(y_train)\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    print('[INFO] Started to train the model.')\n",
    "    print('Training the model on {}.'.format('GPU' if (device == torch.device('cuda') or device == torch.device('mps')) else 'CPU'))\n",
    "#     Start training the model\n",
    "    for ep in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0.0\n",
    "\n",
    "        for idx_batch, train_batches in enumerate(zip(x_train_batches, y_train_batches)):\n",
    "            x_train_batch, y_train_batch = train_batches\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_train_batch)\n",
    "            loss = criterion(outputs, y_train_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        train_acc = acc_of(model, x_train, y_train)\n",
    "        test_acc = acc_of(model, x_test, y_test)\n",
    "        \n",
    "        writer.add_scalar('data/accuracy_train', train_acc, ep)\n",
    "        writer.add_scalar('data/accuracy_test', test_acc, ep)\n",
    "        print('Epoch #{:03d} | Time elapsed : {} | Loss : {:.4e} | Accuracy_train : {:.4e} | Accuracy_test : {:.4e}'.format(\n",
    "                ep + 1, time_since(start), current_loss, train_acc, test_acc))\n",
    "\n",
    "    print('[INFO] Finished training the model. Total time : {}.'.format(time_since(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data('dhg_data.pckl')\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "# instantiation Network\n",
    "model = HandGestureNet(n_channels=66, n_classes=14)\n",
    "# Loss function & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Started to train the model.\n",
      "Training the model on GPU.\n",
      "Epoch #001 | Time elapsed : 00m 38s | Loss : 3.5766e+02 | Accuracy_train : 3.3277e-01 | Accuracy_test : 3.4286e-01\n",
      "Epoch #002 | Time elapsed : 01m 15s | Loss : 1.1782e+02 | Accuracy_train : 5.9874e-01 | Accuracy_test : 6.1190e-01\n",
      "Epoch #003 | Time elapsed : 01m 54s | Loss : 8.1381e+01 | Accuracy_train : 7.3235e-01 | Accuracy_test : 7.3333e-01\n",
      "Epoch #004 | Time elapsed : 02m 32s | Loss : 6.0906e+01 | Accuracy_train : 8.0630e-01 | Accuracy_test : 7.9286e-01\n",
      "Epoch #005 | Time elapsed : 03m 10s | Loss : 4.9835e+01 | Accuracy_train : 8.3571e-01 | Accuracy_test : 8.1905e-01\n",
      "Epoch #006 | Time elapsed : 03m 48s | Loss : 4.1625e+01 | Accuracy_train : 8.4496e-01 | Accuracy_test : 8.1905e-01\n",
      "Epoch #007 | Time elapsed : 04m 26s | Loss : 3.5350e+01 | Accuracy_train : 8.7479e-01 | Accuracy_test : 8.4762e-01\n",
      "Epoch #008 | Time elapsed : 05m 04s | Loss : 2.9909e+01 | Accuracy_train : 9.0546e-01 | Accuracy_test : 8.7381e-01\n",
      "Epoch #009 | Time elapsed : 05m 42s | Loss : 2.6552e+01 | Accuracy_train : 9.2101e-01 | Accuracy_test : 8.7381e-01\n",
      "Epoch #010 | Time elapsed : 06m 19s | Loss : 2.4055e+01 | Accuracy_train : 9.3277e-01 | Accuracy_test : 9.0000e-01\n",
      "Epoch #011 | Time elapsed : 06m 57s | Loss : 2.0939e+01 | Accuracy_train : 9.3529e-01 | Accuracy_test : 8.9286e-01\n",
      "Epoch #012 | Time elapsed : 07m 34s | Loss : 1.9719e+01 | Accuracy_train : 9.3950e-01 | Accuracy_test : 9.0000e-01\n",
      "Epoch #013 | Time elapsed : 08m 11s | Loss : 1.7492e+01 | Accuracy_train : 9.2479e-01 | Accuracy_test : 8.7381e-01\n",
      "Epoch #014 | Time elapsed : 08m 47s | Loss : 1.6733e+01 | Accuracy_train : 9.3992e-01 | Accuracy_test : 9.0000e-01\n",
      "Epoch #015 | Time elapsed : 09m 24s | Loss : 1.5432e+01 | Accuracy_train : 9.2059e-01 | Accuracy_test : 8.8095e-01\n",
      "Epoch #016 | Time elapsed : 10m 00s | Loss : 1.5819e+01 | Accuracy_train : 9.5168e-01 | Accuracy_test : 9.0476e-01\n",
      "Epoch #017 | Time elapsed : 10m 37s | Loss : 1.2828e+01 | Accuracy_train : 9.2521e-01 | Accuracy_test : 8.7857e-01\n",
      "Epoch #018 | Time elapsed : 11m 14s | Loss : 1.2686e+01 | Accuracy_train : 9.5840e-01 | Accuracy_test : 9.0238e-01\n",
      "Epoch #019 | Time elapsed : 11m 50s | Loss : 1.0784e+01 | Accuracy_train : 9.6429e-01 | Accuracy_test : 9.0714e-01\n",
      "Epoch #020 | Time elapsed : 12m 27s | Loss : 1.0048e+01 | Accuracy_train : 9.6471e-01 | Accuracy_test : 9.0238e-01\n",
      "[INFO] Finished training the model. Total time : 12m 27s.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "train(model=model, criterion=criterion, optimizer=optimizer, x_train=x_train, y_train=y_train_14, x_test=x_test, y_test=y_test_14,num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gesture_pretrained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.genfromtxt('GestureData.csv', delimiter=',')\n",
    "\n",
    "data_processed = numpy.zeros((100, 66))\n",
    "\n",
    "for i in range(len(data_processed)):\n",
    "    j = i*22\n",
    "    data_subset = numpy.concatenate(data[j:j+22])\n",
    "    data_processed[i] = data_subset\n",
    "\n",
    "data_processed = data_processed[numpy.newaxis, :]\n",
    "gesture_batch = torch.from_numpy(data_processed)\n",
    "gesture_batch = gesture_batch.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted gesture classes: [12]\n"
     ]
    }
   ],
   "source": [
    "model = HandGestureNet(n_channels=66, n_classes=14)\n",
    "model.load_state_dict(torch.load('gesture_pretrained_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(gesture_batch)\n",
    "    _, predictions = predictions.max(dim=1)\n",
    "    print(\"Predicted gesture classes: {}\".format(predictions.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
