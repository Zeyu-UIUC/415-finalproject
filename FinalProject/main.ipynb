{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from scipy import ndimage as ndimage\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this section, we will be implementing a neural network model for gesture detection. The input to the model will be a\\ntensor and shape (batch_size, duration, n_channels). Each hand skeleton will have 22 joints, and each joint will be 3*number of joints\\nchannels over the time.\\n\\nTo extract features from the input data, we will first process each channel separately. We will use 1D convolutions to\\nprocess each channel, and the neural network will consist of three convolutional layers and pooling layers.\\n\\nThe output of each convolutional layer will be concatenated into a single output, which will be used as input for the next \\nlayer. Finally, the three outputs will be concatenated into one output.\\n\\nThe neural network architecture for this model can be summarized as follows:\\n\\nInput layer with shape (batch_size, duration, n_channels)\\nThree 1D convolutional layers with padding, followed by a max pooling layer\\nThree output layers for each channel, with a concatenation layer at the end\\nBy using 1D convolutions and pooling layers, we can extract meaningful features from the\\nhand skeleton data. The concatenation layer at the end allows us to combine the information from each channel \\ninto a single output, which can be used to predict the gesture being performed\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this section, we will be implementing a neural network model for gesture detection. The input to the model will be a\n",
    "tensor and shape (batch_size, duration, n_channels). Each hand skeleton will have 22 joints, and each joint will be 3*number of joints\n",
    "channels over the time.\n",
    "\n",
    "To extract features from the input data, we will first process each channel separately. We will use 1D convolutions to\n",
    "process each channel, and the neural network will consist of three convolutional layers and pooling layers.\n",
    "\n",
    "The output of each convolutional layer will be concatenated into a single output, which will be used as input for the next \n",
    "layer. Finally, the three outputs will be concatenated into one output.\n",
    "\n",
    "The neural network architecture for this model can be summarized as follows:\n",
    "\n",
    "Input layer with shape (batch_size, duration, n_channels)\n",
    "Three 1D convolutional layers with padding, followed by a max pooling layer\n",
    "Three output layers for each channel, with a concatenation layer at the end\n",
    "By using 1D convolutions and pooling layers, we can extract meaningful features from the\n",
    "hand skeleton data. The concatenation layer at the end allows us to combine the information from each channel \n",
    "into a single output, which can be used to predict the gesture being performed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-processed data\n",
    "def load_data(filepath='dhg_data.pckl'):\n",
    "    file = open(filepath, 'rb')\n",
    "    data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
    "    file.close()\n",
    "    return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']\n",
    "# helper function for preprocess_data()\n",
    "def resize_sequences_length(x_train, x_test, final_length=100):\n",
    "    x_train = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1))]).T for x_i in x_train])\n",
    "    x_test  = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1)) ]).T for x_i in x_test])\n",
    "    return x_train, x_test\n",
    "# helper function for preprocess_data()\n",
    "def shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    x_train, y_train_14, y_train_28 = shuffle(x_train, y_train_14, y_train_28)\n",
    "    x_test,  y_test_14,  y_test_28  = shuffle(x_test,  y_test_14,  y_test_28)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28\n",
    "# shuffle and resize data\n",
    "def preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "    x_train, x_test = resize_sequences_length(x_train, x_test, final_length=100)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28\n",
    "# convert to tensor type for pytorch\n",
    "def convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = numpy.array(y_train_14), numpy.array(y_train_28), numpy.array(y_test_14), numpy.array(y_test_28)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14 - 1, y_train_28 - 1, y_test_14 - 1, y_test_28 - 1\n",
    "    x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = torch.from_numpy(y_train_14), torch.from_numpy(y_train_28), torch.from_numpy(y_test_14), torch.from_numpy(y_test_28)\n",
    "    x_train, x_test = x_train.type(torch.FloatTensor), x_test.type(torch.FloatTensor)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14.type(torch.LongTensor), y_train_28.type(torch.LongTensor), y_test_14.type(torch.LongTensor), y_test_28.type(torch.LongTensor)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandGestureNet(torch.nn.Module):    \n",
    "    \"\"\"\n",
    "    citation:\n",
    "    ------------\n",
    "        @inproceedings{devineau2018deep,\n",
    "            title={Deep learning for hand gesture recognition on skeletal data},\n",
    "            author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
    "            booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
    "            pages={106--113},\n",
    "            year={2018},\n",
    "            organization={IEEE}\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=66, n_classes=14, dropout_probability=0.2):\n",
    "\n",
    "        super(HandGestureNet, self).__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_probability = dropout_probability\n",
    "\n",
    "        self.all_conv_high = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_conv_low = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_residual = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=9 * n_channels * 12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
    "        )\n",
    "\n",
    "        for module in itertools.chain(self.all_conv_high, self.all_conv_low, self.all_residual):\n",
    "            for layer in module:\n",
    "                if layer.__class__.__name__ == \"Conv1d\":\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                    torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "        for layer in self.fc:\n",
    "            if layer.__class__.__name__ == \"Linear\":\n",
    "                torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        all_features = []\n",
    "\n",
    "        for channel in range(0, self.n_channels):\n",
    "            input_channel = input[:, :, channel]\n",
    "            input_channel = input_channel.unsqueeze(1)\n",
    "            high = self.all_conv_high[channel](input_channel)\n",
    "            low = self.all_conv_low[channel](input_channel)\n",
    "            ap_residual = self.all_residual[channel](input_channel)\n",
    "\n",
    "            output_channel = torch.cat([high,low,ap_residual], dim=1)\n",
    "            all_features.append(output_channel)\n",
    "\n",
    "        all_features = torch.cat(all_features, dim=1)\n",
    "        all_features = all_features.view(-1, 9 * self.n_channels * 12)  # <-- 12: depends of the initial sequence length (100).\n",
    "        output = self.fc(all_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(tensor, batch_size=32):\n",
    "    tensor_list = []\n",
    "    length = tensor.shape[0]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if (i + 1) * batch_size >= length:\n",
    "            tensor_list.append(tensor[i * batch_size: length])\n",
    "            return tensor_list\n",
    "        tensor_list.append(tensor[i * batch_size: (i + 1) * batch_size])\n",
    "        i += 1\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '{:02d}m {:02d}s'.format(int(m), int(s))\n",
    "\n",
    "def get_accuracy(model, x, y_ref):\n",
    "    acc = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted = model(x)\n",
    "        _, predicted = predicted.max(dim=1)\n",
    "        acc = 1.0 * (predicted == y_ref).sum().item() / y_ref.shape[0]\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "def train(model, criterion, optimizer,\n",
    "          x_train, y_train, x_test, y_test,\n",
    "          force_cpu=False, num_epochs=5):\n",
    "    \n",
    "    # Check if using a GPU\n",
    "    if torch.cuda.is_available() and not force_cpu:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.has_mps:\n",
    "        device = torch.device('mps')\n",
    "    else: \n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    x_train, y_train, x_test, y_test = x_train.to(device), y_train.to(device), x_test.to(device), y_test.to(device)\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # Prepare all mini-batches\n",
    "    x_train_batches = batch(x_train)\n",
    "    y_train_batches = batch(y_train)\n",
    "    \n",
    "    # Training starting time\n",
    "    start = time.time()\n",
    "\n",
    "    print('[INFO] Started to train the model.')\n",
    "    print('Training the model on {}.'.format('GPU' if (device == torch.device('cuda') or device == torch.device('mps')) else 'CPU'))\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "\n",
    "        # Ensure we're still in training mode\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0.0\n",
    "\n",
    "        for idx_batch, train_batches in enumerate(zip(x_train_batches, y_train_batches)):\n",
    "\n",
    "            # get a mini-batch of sequences\n",
    "            x_train_batch, y_train_batch = train_batches\n",
    "\n",
    "            # zero the gradient parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(x_train_batch)\n",
    "\n",
    "            # backward + optimize\n",
    "            # backward\n",
    "            loss = criterion(outputs, y_train_batch)\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            # for an easy access\n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        train_acc = get_accuracy(model, x_train, y_train)\n",
    "        test_acc = get_accuracy(model, x_test, y_test)\n",
    "        \n",
    "        writer.add_scalar('data/accuracy_train', train_acc, ep)\n",
    "        writer.add_scalar('data/accuracy_test', test_acc, ep)\n",
    "        print('Epoch #{:03d} | Time elapsed : {} | Loss : {:.4e} | Accuracy_train : {:.4e} | Accuracy_test : {:.4e}'.format(\n",
    "                ep + 1, time_since(start), current_loss, train_acc, test_acc))\n",
    "\n",
    "    print('[INFO] Finished training the model. Total time : {}.'.format(time_since(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data()\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "\n",
    "# Network instantiation\n",
    "model = HandGestureNet(n_channels=66, n_classes=14)\n",
    "\n",
    "# Loss function & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Started to train the model.\n",
      "Training the model on GPU.\n",
      "Epoch #001 | Time elapsed : 00m 39s | Loss : 3.3114e+02 | Accuracy_train : 3.6387e-01 | Accuracy_test : 3.3810e-01\n",
      "Epoch #002 | Time elapsed : 01m 18s | Loss : 1.2026e+02 | Accuracy_train : 6.1261e-01 | Accuracy_test : 5.5238e-01\n",
      "Epoch #003 | Time elapsed : 01m 58s | Loss : 8.0129e+01 | Accuracy_train : 7.8529e-01 | Accuracy_test : 7.1429e-01\n",
      "Epoch #004 | Time elapsed : 02m 36s | Loss : 6.0750e+01 | Accuracy_train : 8.0420e-01 | Accuracy_test : 7.4762e-01\n",
      "Epoch #005 | Time elapsed : 03m 15s | Loss : 5.0638e+01 | Accuracy_train : 8.3908e-01 | Accuracy_test : 7.7857e-01\n",
      "Epoch #006 | Time elapsed : 03m 55s | Loss : 4.2241e+01 | Accuracy_train : 8.7017e-01 | Accuracy_test : 8.1190e-01\n",
      "Epoch #007 | Time elapsed : 04m 34s | Loss : 3.5106e+01 | Accuracy_train : 8.8193e-01 | Accuracy_test : 8.3095e-01\n",
      "Epoch #008 | Time elapsed : 05m 14s | Loss : 3.0777e+01 | Accuracy_train : 8.9412e-01 | Accuracy_test : 8.5000e-01\n",
      "Epoch #009 | Time elapsed : 05m 51s | Loss : 2.7539e+01 | Accuracy_train : 8.8739e-01 | Accuracy_test : 8.5000e-01\n",
      "Epoch #010 | Time elapsed : 06m 29s | Loss : 2.6653e+01 | Accuracy_train : 8.7773e-01 | Accuracy_test : 8.2143e-01\n",
      "Epoch #011 | Time elapsed : 07m 09s | Loss : 2.5488e+01 | Accuracy_train : 8.8739e-01 | Accuracy_test : 8.2857e-01\n",
      "Epoch #012 | Time elapsed : 07m 47s | Loss : 2.2462e+01 | Accuracy_train : 8.8319e-01 | Accuracy_test : 8.2381e-01\n",
      "Epoch #013 | Time elapsed : 08m 24s | Loss : 2.0797e+01 | Accuracy_train : 8.9664e-01 | Accuracy_test : 8.2857e-01\n",
      "Epoch #014 | Time elapsed : 09m 02s | Loss : 1.9949e+01 | Accuracy_train : 9.0546e-01 | Accuracy_test : 8.3810e-01\n",
      "Epoch #015 | Time elapsed : 09m 40s | Loss : 1.8104e+01 | Accuracy_train : 8.7731e-01 | Accuracy_test : 8.1429e-01\n",
      "Epoch #016 | Time elapsed : 10m 18s | Loss : 1.7570e+01 | Accuracy_train : 9.3235e-01 | Accuracy_test : 8.5476e-01\n",
      "Epoch #017 | Time elapsed : 10m 59s | Loss : 1.4987e+01 | Accuracy_train : 8.5882e-01 | Accuracy_test : 7.7619e-01\n",
      "Epoch #018 | Time elapsed : 11m 36s | Loss : 1.4519e+01 | Accuracy_train : 9.1176e-01 | Accuracy_test : 8.2857e-01\n",
      "Epoch #019 | Time elapsed : 12m 15s | Loss : 1.5256e+01 | Accuracy_train : 9.3403e-01 | Accuracy_test : 8.5238e-01\n",
      "Epoch #020 | Time elapsed : 12m 54s | Loss : 1.1835e+01 | Accuracy_train : 9.5252e-01 | Accuracy_test : 8.7619e-01\n",
      "Epoch #021 | Time elapsed : 13m 33s | Loss : 1.2333e+01 | Accuracy_train : 9.5630e-01 | Accuracy_test : 8.7857e-01\n",
      "Epoch #022 | Time elapsed : 14m 12s | Loss : 1.0695e+01 | Accuracy_train : 9.7227e-01 | Accuracy_test : 8.9524e-01\n",
      "Epoch #023 | Time elapsed : 14m 52s | Loss : 9.3619e+00 | Accuracy_train : 9.7395e-01 | Accuracy_test : 9.0952e-01\n",
      "Epoch #024 | Time elapsed : 15m 31s | Loss : 9.2493e+00 | Accuracy_train : 9.6303e-01 | Accuracy_test : 8.9524e-01\n",
      "Epoch #025 | Time elapsed : 16m 11s | Loss : 9.1770e+00 | Accuracy_train : 9.6681e-01 | Accuracy_test : 8.9762e-01\n",
      "Epoch #026 | Time elapsed : 16m 51s | Loss : 8.9803e+00 | Accuracy_train : 9.7017e-01 | Accuracy_test : 9.0952e-01\n",
      "Epoch #027 | Time elapsed : 17m 30s | Loss : 7.5024e+00 | Accuracy_train : 9.7563e-01 | Accuracy_test : 9.1190e-01\n",
      "Epoch #028 | Time elapsed : 18m 07s | Loss : 7.0980e+00 | Accuracy_train : 9.8067e-01 | Accuracy_test : 9.1429e-01\n",
      "Epoch #029 | Time elapsed : 18m 46s | Loss : 7.7677e+00 | Accuracy_train : 9.6807e-01 | Accuracy_test : 9.0714e-01\n",
      "Epoch #030 | Time elapsed : 19m 24s | Loss : 7.4019e+00 | Accuracy_train : 9.6218e-01 | Accuracy_test : 9.0000e-01\n",
      "Epoch #031 | Time elapsed : 20m 02s | Loss : 6.6370e+00 | Accuracy_train : 9.7899e-01 | Accuracy_test : 9.0238e-01\n",
      "Epoch #032 | Time elapsed : 20m 40s | Loss : 5.6266e+00 | Accuracy_train : 9.7773e-01 | Accuracy_test : 9.1429e-01\n",
      "Epoch #033 | Time elapsed : 21m 18s | Loss : 6.0365e+00 | Accuracy_train : 9.8655e-01 | Accuracy_test : 9.2143e-01\n",
      "Epoch #034 | Time elapsed : 21m 57s | Loss : 5.5085e+00 | Accuracy_train : 9.9034e-01 | Accuracy_test : 9.2619e-01\n",
      "Epoch #035 | Time elapsed : 22m 35s | Loss : 5.1877e+00 | Accuracy_train : 9.7857e-01 | Accuracy_test : 9.0714e-01\n",
      "Epoch #036 | Time elapsed : 23m 13s | Loss : 4.8787e+00 | Accuracy_train : 9.9118e-01 | Accuracy_test : 9.2381e-01\n",
      "Epoch #037 | Time elapsed : 23m 51s | Loss : 6.5151e+00 | Accuracy_train : 9.7857e-01 | Accuracy_test : 9.1429e-01\n",
      "Epoch #038 | Time elapsed : 24m 28s | Loss : 5.4459e+00 | Accuracy_train : 9.9244e-01 | Accuracy_test : 9.2619e-01\n",
      "Epoch #039 | Time elapsed : 25m 06s | Loss : 4.2811e+00 | Accuracy_train : 9.8319e-01 | Accuracy_test : 9.0714e-01\n",
      "Epoch #040 | Time elapsed : 25m 43s | Loss : 3.8137e+00 | Accuracy_train : 9.6933e-01 | Accuracy_test : 8.9048e-01\n",
      "[INFO] Finished training the model. Total time : 25m 43s.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "num_epochs = 40\n",
    "train(model=model, criterion=criterion, optimizer=optimizer, x_train=x_train, y_train=y_train_14, x_test=x_test, y_test=y_test_14,num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gesture_pretrained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the data from unity leap motion\n",
    "def unityProcess(unitydata):\n",
    "    data_processed = numpy.zeros((100, 66))\n",
    "\n",
    "    for i in range(len(data_processed)):\n",
    "        j = i*22\n",
    "        data_subset = numpy.concatenate(unitydata[j:j+22])\n",
    "        data_processed[i] = data_subset\n",
    "\n",
    "    data_processed = data_processed[numpy.newaxis, :]\n",
    "    gesture_batch = torch.from_numpy(data_processed)\n",
    "    gesture_batch = gesture_batch.type(torch.FloatTensor)\n",
    "    return gesture_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted gesture classes: [4]\n"
     ]
    }
   ],
   "source": [
    "model = HandGestureNet(n_channels=66, n_classes=14)\n",
    "model.load_state_dict(torch.load('gesture_pretrained_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# make predictions\n",
    "with torch.no_grad():\n",
    "    unitydata = numpy.genfromtxt('GestureData.csv', delimiter=',')\n",
    "    gesture_batch = unityProcess(unitydata)\n",
    "    predictions = model(gesture_batch)\n",
    "    _, predictions = predictions.max(dim=1)\n",
    "    print(\"Predicted gesture classes: {}\".format(predictions.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
