{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from scipy import ndimage as ndimage\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From paper\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except:\n",
    "    class SummaryWriter():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy\n",
    "import pickle\n",
    "from scipy import ndimage as ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\"\"\n",
    "    Data set citation: \n",
    "    Dynamic Hand Gesture Recognition using Skeleton-based Features ,\n",
    "    Quentin De Smedt, Hazem Wannous and Jean-Philippe Vandeborre, \n",
    "    2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).\n",
    "    Download from http://www-rech.telecom-lille.fr/DHGdataset/ and unzip into ./dataset_shrec\n",
    "\"\"\"\"\n",
    "def resize_gestures(input_gestures, final_length=100):\n",
    "    output_gestures = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1))]).T for x_i in input_gestures])\n",
    "    return output_gestures\n",
    "\n",
    "\n",
    "def load_gestures(dataset, root, version_x, resize_gesture_to_length):\n",
    "    if version_x == '3D':\n",
    "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeletons_world.txt'\n",
    "    else:\n",
    "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeletons_image.txt'\n",
    "\n",
    "    gestures_filenames = sorted(glob.glob(pattern))\n",
    "    gestures = [numpy.genfromtxt(f) for f in gestures_filenames]\n",
    "    if resize_gesture_to_length is not None:\n",
    "        gestures = resize_gestures(gestures, final_length=resize_gesture_to_length)\n",
    "#     print(filename.split('/') for filename in gestures_filenames)\n",
    "    labels_14 = [int(filename.split('/')[-5].split('_')[1]) for filename in gestures_filenames]\n",
    "    labels_28 = [int(filename.split('/')[-4].split('_')[1]) for filename in gestures_filenames]\n",
    "    labels_28 = [labels_14[idx_gesture] if n_fingers_used == 1 else 14 + labels_14[idx_gesture] for idx_gesture, n_fingers_used in enumerate(labels_28)]\n",
    "    return gestures, labels_14, labels_28\n",
    "\n",
    "\n",
    "def write_data(data, filepath):\n",
    "    with open(filepath, 'wb') as output_file:\n",
    "        pickle.dump(data, output_file)\n",
    "\n",
    "\n",
    "# def load_data(filepath):\n",
    "#     file = open(filepath, 'rb')\n",
    "#     data = pickle.load(file, encoding='latin1')  #could be'utf8'\n",
    "#     file.close()\n",
    "#     return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures, labels_14, labels_28 = load_gestures(dataset='shrec',\n",
    "                                               root='dataset_shrec',\n",
    "                                               version_x='3D',\n",
    "                                               resize_gesture_to_length=100)\n",
    "# Split the dataset into train and test sets if you want:\n",
    "x_train, x_test, y_train_14, y_test_14, y_train_28, y_test_28 = train_test_split(gestures, labels_14, labels_28, test_size=0.15)\n",
    "\n",
    "# print(len(x_train), len(y_train_14), len(y_train_28), len(x_test), len(y_test_14), len(y_test_28) )\n",
    "\n",
    "# Save the dataset\n",
    "data = {\n",
    "    'x_train': x_train,\n",
    "    'x_test': x_test,\n",
    "    'y_train_14': y_train_14,\n",
    "    'y_train_28': y_train_28,\n",
    "    'y_test_14': y_test_14,\n",
    "    'y_test_28': y_test_28\n",
    "}\n",
    "write_data(data, filepath='dhg_data.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath='dhg_data.pckl'):\n",
    "    file = open(filepath, 'rb')\n",
    "    data = pickle.load(file, encoding='latin1') \n",
    "    file.close()\n",
    "    return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_sequences_length(x_train, x_test, final_length=100):\n",
    "    x_train = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1))]).T for x_i in x_train])\n",
    "    x_test  = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1)) ]).T for x_i in x_test])\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    x_train, y_train_14, y_train_28 = shuffle(x_train, y_train_14, y_train_28)\n",
    "    x_test,  y_test_14,  y_test_28  = shuffle(x_test,  y_test_14,  y_test_28)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "    x_train, x_test = resize_sequences_length(x_train, x_test, final_length=100)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    # as numpy\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = numpy.array(y_train_14), numpy.array(y_train_28), numpy.array(y_test_14), numpy.array(y_test_28)\n",
    "    \n",
    "    # -- REQUIRED by the pytorch loss function implementation --\n",
    "    # Remove 1 to all classes items (1-14 => 0-13 and 1-28 => 0-27)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14 - 1, y_train_28 - 1, y_test_14 - 1, y_test_28 - 1\n",
    "    \n",
    "    # as torch\n",
    "    x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = torch.from_numpy(y_train_14), torch.from_numpy(y_train_28), torch.from_numpy(y_test_14), torch.from_numpy(y_test_28)\n",
    "\n",
    "    # -- REQUIRED by the pytorch loss function implementation --\n",
    "    # correct the data type (for the loss function used)\n",
    "    x_train, x_test = x_train.type(torch.FloatTensor), x_test.type(torch.FloatTensor)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14.type(torch.LongTensor), y_train_28.type(torch.LongTensor), y_test_14.type(torch.LongTensor), y_test_28.type(torch.LongTensor)\n",
    "    \n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Misc.\n",
    "# -------------\n",
    "def batch(tensor, batch_size=32):\n",
    "    \"\"\"Return a list of (mini) batches\"\"\"\n",
    "    tensor_list = []\n",
    "    length = tensor.shape[0]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if (i + 1) * batch_size >= length:\n",
    "            tensor_list.append(tensor[i * batch_size: length])\n",
    "            return tensor_list\n",
    "        tensor_list.append(tensor[i * batch_size: (i + 1) * batch_size])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '{:02d}m {:02d}s'.format(int(m), int(s))\n",
    "\n",
    "\n",
    "def get_accuracy(model, x, y_ref):\n",
    "    \"\"\"Get the accuracy of the pytorch model on a batch\"\"\"\n",
    "    acc = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted = model(x)\n",
    "        _, predicted = predicted.max(dim=1)\n",
    "        acc = 1.0 * (predicted == y_ref).sum().item() / y_ref.shape[0]\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandGestureNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    citation:\n",
    "    ------------\n",
    "        @inproceedings{devineau2018deep,\n",
    "            title={Deep learning for hand gesture recognition on skeletal data},\n",
    "            author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
    "            booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
    "            pages={106--113},\n",
    "            year={2018},\n",
    "            organization={IEEE}\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels=66, n_classes=14, dropout_probability=0.2):\n",
    "\n",
    "        super(HandGestureNet, self).__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_probability = dropout_probability\n",
    "\n",
    "        # Layers ----------------------------------------------\n",
    "        self.all_conv_high = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_conv_low = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_residual = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=9 * n_channels * 12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
    "        )\n",
    "\n",
    "        # Initialization --------------------------------------\n",
    "        # Xavier init\n",
    "        for module in itertools.chain(self.all_conv_high, self.all_conv_low, self.all_residual):\n",
    "            for layer in module:\n",
    "                if layer.__class__.__name__ == \"Conv1d\":\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                    torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "        for layer in self.fc:\n",
    "            if layer.__class__.__name__ == \"Linear\":\n",
    "                torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # Work on each channel separately\n",
    "        all_features = []\n",
    "\n",
    "        for channel in range(0, self.n_channels):\n",
    "            input_channel = input[:, :, channel]\n",
    "\n",
    "            # Add a dummy (spatial) dimension for the time convolutions\n",
    "            # Conv1D format : (batch_size, n_feature_maps, duration)\n",
    "            input_channel = input_channel.unsqueeze(1)\n",
    "\n",
    "            high = self.all_conv_high[channel](input_channel)\n",
    "            low = self.all_conv_low[channel](input_channel)\n",
    "            ap_residual = self.all_residual[channel](input_channel)\n",
    "\n",
    "            # Time convolutions are concatenated along the feature maps axis\n",
    "            output_channel = torch.cat([\n",
    "                high,\n",
    "                low,\n",
    "                ap_residual\n",
    "            ], dim=1)\n",
    "            all_features.append(output_channel)\n",
    "\n",
    "        # Concatenate along the feature maps axis\n",
    "        all_features = torch.cat(all_features, dim=1)\n",
    "        \n",
    "        # Flatten for the Linear layers\n",
    "        all_features = all_features.view(-1, 9 * self.n_channels * 12)  # <-- 12: depends of the initial sequence length (100).\n",
    "        # If you have shorter/longer sequences, you probably do NOT even need to modify the modify the network architecture:\n",
    "        # resampling your input gesture from T timesteps to 100 timesteps will (surprisingly) probably actually work as well!\n",
    "\n",
    "        # Fully-Connected Layers\n",
    "        output = self.fc(all_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data()\n",
    "\n",
    "# Shuffle sequences and resize sequences\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "\n",
    "# Convert to pytorch variables\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandGestureNet(n_channels=66, n_classes=14)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer,\n",
    "          x_train, y_train, x_test, y_test,\n",
    "          force_cpu=False, num_epochs=5):\n",
    "    \n",
    "    # from the paper\n",
    "    if torch.cuda.is_available() and not force_cpu:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.has_mps:\n",
    "        device = torch.device('mps')\n",
    "    else: \n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    x_train, y_train, x_test, y_test = x_train.to(device), y_train.to(device), x_test.to(device), y_test.to(device)\n",
    "    \n",
    "    # (bonus) log accuracy values to visualize them in tensorboard:\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # Prepare all mini-batches\n",
    "    x_train_batches = batch(x_train)\n",
    "    y_train_batches = batch(y_train)\n",
    "    \n",
    "    # Training starting time\n",
    "    start = time.time()\n",
    "\n",
    "    print('[INFO] Started to train the model.')\n",
    "    print('Training the model on {}.'.format('GPU' if (device == torch.device('cuda') or device == torch.device('mps')) else 'CPU'))\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "\n",
    "        # Ensure we're still in training mode\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0.0\n",
    "\n",
    "        for idx_batch, train_batches in enumerate(zip(x_train_batches, y_train_batches)):\n",
    "\n",
    "            # get a mini-batch of sequences\n",
    "            x_train_batch, y_train_batch = train_batches\n",
    "\n",
    "            # zero the gradient parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(x_train_batch)\n",
    "\n",
    "            # backward + optimize\n",
    "            # backward\n",
    "            loss = criterion(outputs, y_train_batch)\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            # for an easy access\n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        train_acc = get_accuracy(model, x_train, y_train)\n",
    "        test_acc = get_accuracy(model, x_test, y_test)\n",
    "        \n",
    "        writer.add_scalar('data/accuracy_train', train_acc, ep)\n",
    "        writer.add_scalar('data/accuracy_test', test_acc, ep)\n",
    "        print('Epoch #{:03d} | Time elapsed : {} | Loss : {:.4e} | Accuracy_train : {:.4e} | Accuracy_test : {:.4e}'.format(\n",
    "                ep + 1, time_since(start), current_loss, train_acc, test_acc))\n",
    "\n",
    "    print('[INFO] Finished training the model. Total time : {}.'.format(time_since(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Started to train the model.\n",
      "Training the model on CPU.\n",
      "Epoch #001 | Time elapsed : 02m 35s | Loss : 3.3115e+02 | Accuracy_train : 3.2689e-01 | Accuracy_test : 3.1667e-01\n",
      "Epoch #002 | Time elapsed : 05m 09s | Loss : 1.2053e+02 | Accuracy_train : 5.1639e-01 | Accuracy_test : 4.4524e-01\n",
      "Epoch #003 | Time elapsed : 07m 54s | Loss : 8.6311e+01 | Accuracy_train : 7.2605e-01 | Accuracy_test : 6.4762e-01\n",
      "Epoch #004 | Time elapsed : 10m 48s | Loss : 6.3097e+01 | Accuracy_train : 7.9328e-01 | Accuracy_test : 7.1667e-01\n",
      "Epoch #005 | Time elapsed : 13m 30s | Loss : 5.0130e+01 | Accuracy_train : 8.3025e-01 | Accuracy_test : 7.4048e-01\n",
      "Epoch #006 | Time elapsed : 16m 35s | Loss : 4.3560e+01 | Accuracy_train : 8.5042e-01 | Accuracy_test : 7.8333e-01\n",
      "Epoch #007 | Time elapsed : 19m 18s | Loss : 3.7319e+01 | Accuracy_train : 8.7437e-01 | Accuracy_test : 8.1667e-01\n",
      "Epoch #008 | Time elapsed : 22m 09s | Loss : 3.2202e+01 | Accuracy_train : 9.0420e-01 | Accuracy_test : 8.4524e-01\n",
      "Epoch #009 | Time elapsed : 24m 51s | Loss : 2.8564e+01 | Accuracy_train : 9.1218e-01 | Accuracy_test : 8.7143e-01\n",
      "Epoch #010 | Time elapsed : 27m 37s | Loss : 2.5529e+01 | Accuracy_train : 9.1765e-01 | Accuracy_test : 8.7619e-01\n",
      "Epoch #011 | Time elapsed : 30m 26s | Loss : 2.2935e+01 | Accuracy_train : 9.1723e-01 | Accuracy_test : 8.7381e-01\n",
      "Epoch #012 | Time elapsed : 33m 31s | Loss : 2.1980e+01 | Accuracy_train : 9.1176e-01 | Accuracy_test : 8.7619e-01\n",
      "Epoch #013 | Time elapsed : 36m 28s | Loss : 2.1350e+01 | Accuracy_train : 9.2941e-01 | Accuracy_test : 8.8571e-01\n",
      "Epoch #014 | Time elapsed : 39m 20s | Loss : 1.9156e+01 | Accuracy_train : 9.1176e-01 | Accuracy_test : 8.5952e-01\n",
      "Epoch #015 | Time elapsed : 42m 08s | Loss : 1.6812e+01 | Accuracy_train : 9.0966e-01 | Accuracy_test : 8.5000e-01\n",
      "Epoch #016 | Time elapsed : 44m 57s | Loss : 1.6905e+01 | Accuracy_train : 9.3151e-01 | Accuracy_test : 8.7857e-01\n",
      "Epoch #017 | Time elapsed : 47m 52s | Loss : 1.5218e+01 | Accuracy_train : 9.2143e-01 | Accuracy_test : 8.5714e-01\n",
      "Epoch #018 | Time elapsed : 50m 39s | Loss : 1.5217e+01 | Accuracy_train : 9.3487e-01 | Accuracy_test : 8.7143e-01\n",
      "Epoch #019 | Time elapsed : 53m 30s | Loss : 1.3410e+01 | Accuracy_train : 9.5378e-01 | Accuracy_test : 9.0714e-01\n",
      "Epoch #020 | Time elapsed : 56m 17s | Loss : 1.3599e+01 | Accuracy_train : 9.3235e-01 | Accuracy_test : 8.5952e-01\n",
      "[INFO] Finished training the model. Total time : 56m 17s.\n"
     ]
    }
   ],
   "source": [
    "# Please adjust the training epochs count, and the other hyperparams (lr, dropout, ...), for a non-overfitted training according to your own needs.\n",
    "# tip: use tensorboard to display the accuracy (see cells above for tensorboard usage)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "train(model=model, criterion=criterion, optimizer=optimizer,\n",
    "      x_train=x_train, y_train=y_train_14, x_test=x_test, y_test=y_test_14,\n",
    "      num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gesture_pretrained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandGestureNet(\n",
       "  (all_conv_high): ModuleList(\n",
       "    (0-65): 66 x Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (3): Conv1d(8, 4, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (6): Conv1d(4, 4, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "      (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (all_conv_low): ModuleList(\n",
       "    (0-65): 66 x Sequential(\n",
       "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (3): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (6): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "      (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (all_residual): ModuleList(\n",
       "    (0-65): 66 x Sequential(\n",
       "      (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=7128, out_features=1936, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1936, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HandGestureNet(n_channels=66, n_classes=14)\n",
    "model.load_state_dict(torch.load('gesture_pretrained_model.pt'))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected : Grab\n",
      "detected : Grab\n",
      "detected : Grab\n",
      "detected : Grab\n",
      "detected : Grab\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n",
      "detected : Rotation Clockwis\n"
     ]
    }
   ],
   "source": [
    "import keras.utils as image\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# make predictions\n",
    "# with torch.no_grad():\n",
    "# #     demo_gesture_batch = torch.randn(32, 100, 66)\n",
    "# #     img = image.load_img('test1.jpg', target_size=(224,224))\n",
    "# #     image_array = image.img_to_array(img)\n",
    "# #     img_batch = np.expand_dims(image_array, axis=0)\n",
    "# #     img_preprocessed = preprocess_input(img_batch)\n",
    "# #     predictions = model(32,100,66)\n",
    "# #     _, predictions = predictions.max(dim=1)\n",
    "#     print(\"Predicted gesture classes: {}\".format(predictions))\n",
    "\n",
    "dict = {\n",
    "    0: \"Grab\",\n",
    "    1: \"Tap\",\n",
    "    2:\"Expand\",\n",
    "    3:\"Pinch\",\n",
    "    4:\"Rotation Clockwis\",\n",
    "    5:\"Rotation Counter Clockwise\",\n",
    "    6:\"Swipe Right\",\n",
    "    7:\"Swipe Left\",\n",
    "    8:\"Swipe Up\",\n",
    "    9:\"Swipe Down\",\n",
    "    10:\"Swipe X\",\n",
    "    11:\"Swipe +\",\n",
    "    12:\"Swipe V\",\n",
    "    13:\"Shake\"\n",
    "}\n",
    "with torch.no_grad():\n",
    "    demo_gesture_batch = torch.Tensor(32, 100, 66)\n",
    "#     print(demo_gesture_batch)\n",
    "    predictions = model(demo_gesture_batch)\n",
    "    _, predictions = predictions.max(dim=1)\n",
    "    l = predictions.tolist()\n",
    "    for k in l:\n",
    "        print(\"detected :\", dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "# pip install 'pillow<7.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected : Tap\n",
      "detected : Rotation Counter Clockwise\n",
      "detected : Shake\n",
      "detected : Tap\n",
      "detected : Shake\n",
      "detected : Tap\n",
      "detected : Rotation Counter Clockwise\n",
      "detected : Shake\n",
      "detected : Tap\n",
      "detected : Shake\n",
      "detected : Tap\n",
      "detected : Rotation Counter Clockwise\n",
      "detected : Shake\n",
      "detected : Tap\n",
      "detected : Shake\n",
      "detected as: Tap\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Read the image\n",
    "image = Image.open('opencv_frame_0.png')\n",
    "\n",
    "# Define a transform to convert the image to tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Convert the image to PyTorch tensor\n",
    "tensor = transform(image)\n",
    "predictions = model(tensor)\n",
    "_, predictions = predictions.max(dim=1)\n",
    "ll = predictions.tolist()\n",
    "for k in ll:\n",
    "    print(\"detected :\", dict[k])\n",
    "\n",
    "print( \"detected as:\", dict[max(set(ll), key=ll.count)])\n",
    "# print the converted image tensor\n",
    "# print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
